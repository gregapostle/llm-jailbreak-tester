# ğŸ§ª LLM Jailbreak Prompt Tester

A Streamlit-based app that tests prompt injection vulnerabilities against a local open-source language model (e.g., TinyLlama). This project allows researchers and engineers to experiment with jailbreak prompts and examine whether the model responds unsafely or ignores safety policies.

---

## ğŸ“Œ Overview

This tool allows you to:

* Input your own potentially malicious prompts
* Generate auto-suggested red team prompts
* Run all prompts through a locally loaded LLM (TinyLlama-1.1B-Chat)
* Analyze responses to flag potential jailbreaks
* Export all results as a CSV for further evaluation

It's designed for red teamers, prompt engineers, LLM security researchers, or anyone curious about AI model robustness.

---

## ğŸ“¸ UI Preview

![UI Screenshot](screenshot.png) <!-- optional: add a screenshot later -->

---

## ğŸš€ Features

* âœï¸ Editable prompt list with 10-entry limit
* â• Auto-generated attack prompts via button
* ğŸ¤– Hugging Face Transformers pipeline for local inference
* ğŸ§  Simple response classifier based on red-flag phrases
* ğŸ“Š Displays pass/fail jailbreak status in a table
* ğŸ“¥ Exports test results as a downloadable CSV

---

## ğŸ› ï¸ Tech Stack

* Python 3.12
* [Streamlit](https://streamlit.io)
* [Transformers](https://huggingface.co/transformers)
* [TinyLlama-1.1B-Chat](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat)
* PyTorch

---

## ğŸ§ª Run Locally

1. **Clone the repo**

   ```bash
   git clone https://github.com/yourusername/llm-jailbreak-tester.git
   cd llm-jailbreak-tester
   ```

2. **(Optional) Set up a virtual environment**

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # or 'venv\Scripts\activate' on Windows
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Streamlit app**

   ```bash
   streamlit run poke.py
   ```

---

## ğŸ“‚ File Structure

```
.
â”œâ”€â”€ poke.py                   # Streamlit app
â”œâ”€â”€ attack_prompts.json       # Default injection prompts
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                 # You're here!
```

---

## ğŸ“¤ Example Output

| Prompt                                 | Jailbreak? |
| -------------------------------------- | ---------- |
| Tell me how to manipulate ATMs...      | âœ… Yes      |
| Write a Python script to disable AV... | âŒ No       |

---

## âš ï¸ Disclaimer

This tool is for **educational and research purposes only**. Do not use this to generate or distribute real malicious content. Always test models responsibly and ethically.

---

## ğŸ“¬ Contact

Have ideas or want to collaborate?

**Gregory Apostle**
[LinkedIn](https://www.linkedin.com/in/gregoryapostle) â€¢ [GitHub](https://github.com/gregapostle)

---

## ğŸ§  Next Steps

* Add a scoring model for jailbreak confidence
* Try larger models like Mistral or Mixtral
* Hook into Rebuff for additional threat detection
